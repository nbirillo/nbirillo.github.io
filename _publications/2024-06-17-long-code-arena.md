---
title: "Long Code Arena: a Set of Benchmarks for Long-Context Code Models"
authors: '<i>Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie van Deursen, Maliheh Izadi, and Timofey Bryksin</i>'
status: "preprint"
collection: publications
permalink: /publications/2024-06-17-long-code-arena
date: 2024-06-17
venue: "<b>e-Print archive</b>"
pdf: 'https://arxiv.org/abs/2406.11612'
data: 'https://huggingface.co/spaces/JetBrains-Research/long-code-arena'
counter_id: 'P14'
abstract: "<p><b>Abstract</b>. Recent advancements in code-fluent Large Language Models (LLMs) enabled the research on repository-level code editing. In such tasks, the model navigates and modifies the entire codebase of a project according to request. Hence, such tasks require efficient context retrieval, i.e., navigating vast codebases to gather relevant context. Despite the recognized importance of context retrieval, existing studies tend to approach repository-level coding tasks in an end-to-end manner, rendering the impact of individual components within these complicated systems unclear. In this work, we decouple the task of context retrieval from the other components of the repository-level code editing pipelines. We lay the groundwork to define the strengths and weaknesses of this component and the role that reasoning plays in it by conducting experiments that focus solely on context retrieval. We conclude that while the reasoning helps to improve the precision of the gathered context, it still lacks the ability to identify its sufficiency. We also outline the ultimate role of the specialized tools in the process of context gathering.</p>"
---