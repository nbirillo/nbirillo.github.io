---
title: "Analyzing the Quality of Submissions in Online Programming Courses"
authors: '<i>Maria Tigina, Anastasiia Birillo, Yaroslav Golubev, Hieke Keuning, Nikolay Vyahhi, and Timofey Bryksin</i>'
status: "published"
collection: publications
permalink: /publications/2023-05-14-code-quality-analysis
date: 2023-05-14
venue: "the proceedings of <b>ICSE'23</b>"
level: 'A*'
pdf: 'https://arxiv.org/abs/2301.11158'
paperurl: 'https://doi.org/10.1109/ICSE-SEET58685.2023.00031'
data: 'https://zenodo.org/record/7573422'
counter_id: 'C7'
abstract: "<p><b>Abstract</b>. Programming education should aim to provide students with a broad range of skills that they will later use while developing software. An important aspect in this is their ability to write code that is not only correct but also of high quality. Unfortunately, this is difficult to control in the setting of a massive open online course. In this paper, we carry out an analysis of the code quality of submissions from JetBrains Academy - a platform for studying programming in an industry-like project-based setting with an embedded code quality assessment tool called Hyperstyle. We analyzed more than a million Java submissions and more than 1.3 million Python submissions, studied the most prevalent types of code quality issues and the dynamics of how students fix them. We provide several case studies of different issues, as well as an analysis of why certain issues remain unfixed even after several attempts. Also, we studied abnormally long sequences of submissions, in which students attempted to fix code quality issues after passing the task. Our results point the way towards the improvement of online courses, such as making sure that the task itself does not incentivize students to write code poorly.</p>"
---